{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeM5-IE_8fSf",
        "outputId": "405496b7-7563-41bf-de74-88d6f00f150b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 10 16:14:41 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5WwhoHR8qdF",
        "outputId": "264891a9-a063-4d5a-9ed3-8d8aca4eca75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, run this installation command in your Colab notebook to ensure all necessary libraries are installed.\n",
        "!pip install torch transformers[torch] datasets accelerate -U"
      ],
      "metadata": {
        "id": "oReh8rGB6lfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "cXbaX3uvB_P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch transformers[torch] datasets accelerate -U\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oUS_dYiR-5KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments, GPTNeoConfig\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Set PYTORCH_CUDA_ALLOC_CONF to avoid fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Initialize the tokenizer\n",
        "#model_name = 'EleutherAI/gpt-neo-1.3B'\n",
        "model_name = 'EleutherAI/gpt-neo-125m'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Explicitly set the tokenizer's padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialize the model with gradient checkpointing enabled for memory efficiency\n",
        "model_config = GPTNeoConfig.from_pretrained(model_name, gradient_checkpointing=True)\n",
        "model = GPTNeoForCausalLM.from_pretrained(model_name, config=model_config)\n",
        "\n",
        "# Function to tokenize the dataset correctly, considering batched inputs\n",
        "def tokenize_function(examples):\n",
        "    concatenated_messages = [\" \".join([message['content'] for message in example_messages]) for example_messages in examples['messages']]\n",
        "    return tokenizer(concatenated_messages, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "# Paths to your training and testing dataset\n",
        "train_path = '/content/drive/My Drive/Colab_Notebooks/datasets/train_data.jsonl'\n",
        "test_path = '/content/drive/My Drive/Colab_Notebooks/datasets/test_data.jsonl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pul2OB8q-5Zp",
        "outputId": "6c0fa9ff-216c-4c01-e2a5-0adf36eb1563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': load_dataset('json', data_files=train_path, split='train'),\n",
        "    'test': load_dataset('json', data_files=test_path, split='train')\n",
        "})\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=['messages'])\n",
        "\n",
        "# Data collator to dynamically pad the inputs and labels\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Define the training arguments with reduced batch size and enabled mixed precision\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "i3litaHC-5gr",
        "outputId": "2a816ec7-b7f8-4622-f963-5c9420263769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:10, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-2 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=4.233532428741455, metrics={'train_runtime': 11.6745, 'train_samples_per_second': 1.884, 'train_steps_per_second': 0.171, 'total_flos': 979526615040.0, 'train_loss': 4.233532428741455, 'epoch': 1.33})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer for future use\n",
        "model.save_pretrained('/content/drive/My Drive/Colab_Notebooks/fine_tuned_model_gpt_neo')\n",
        "tokenizer.save_pretrained('/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbxu1Epdz1Q5",
        "outputId": "d18a67ee-a6a5-4d3c-e1e7-cd577b15a055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo/vocab.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo/merges.txt',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer for inference\n",
        "model = GPTNeoForCausalLM.from_pretrained('/content/drive/My Drive/Colab_Notebooks/fine_tuned_model_gpt_neo')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt_neo')\n",
        "\n",
        "# Example of user query for inference\n",
        "user_query = \"Tell me something interesting about space.\"\n",
        "\n",
        "# Encode the user query to tensor of input IDs\n",
        "input_ids = tokenizer.encode(user_query, return_tensors='pt')\n",
        "\n",
        "# Generate a sequence of tokens in response to the input query\n",
        "# output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=300,\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    temperature=0.8,  # Lower temperature\n",
        "    top_p=0.92,  # Nucleus sampling\n",
        "    top_k=50,  # Top-k sampling\n",
        "    repetition_penalty=1.2,  # Penalize repetition\n",
        "    no_repeat_ngram_size=2  # Prevent repeating n-grams\n",
        ")\n",
        "\n",
        "# Decode the generated sequence to a human-readable string\n",
        "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "response = tokenizer.decode(\n",
        "    model.generate(\n",
        "        input_ids,\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.8,  # Adjust temperature\n",
        "        top_k=50,  # Top-k sampling\n",
        "        top_p=0.92,  # Nucleus (top-p) sampling\n",
        "        repetition_penalty=1.2,  # Apply repetition penalty\n",
        "        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
        "        do_sample=True  # Enable sampling to use temperature, top_k, and top_p\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(\"Generated response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOd1ApgRz4JB",
        "outputId": "355cadd5-7914-4d1b-ba49-57f1d53ffc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: Tell me something interesting about space.\n",
            "\n",
            "I love the way the planets and stars are portrayed in the movies. But, thereâ€™s a huge problem with the depiction of the universe in those films. For a long time, they were depicted\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}