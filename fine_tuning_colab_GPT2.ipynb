{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Jb15xPmUET"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers[torch] accelerate -U\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained model and tokenizer (GPT2 from Hugging_Face)\n",
        "model_name = 'gpt2'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl7uZ3o3E_E1",
        "outputId": "5238dfe6-a27a-4bd9-e274-376ce01cbe51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for training and testing\n",
        "train_path = '/content/drive/My Drive/Colab_Notebooks/datasets/train_data.jsonl'\n",
        "test_path = '/content/drive/My Drive/Colab_Notebooks/datasets/test_data.jsonl'"
      ],
      "metadata": {
        "id": "cjhTZ-bfE9l2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the training and test files\n",
        "# TextDataset is a utility from transformers that reads and formats the data file for training\n",
        "train_dataset = TextDataset(\n",
        "  tokenizer=tokenizer, # The tokenizer used to process the text\n",
        "  file_path=train_path, # The path to the training data\n",
        "  block_size=128 # The size of the blocks the dataset will be split into for training\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(\n",
        "  tokenizer=tokenizer, # The tokenizer used to process the text\n",
        "  file_path=test_path, # The path to the testing data\n",
        "  block_size=128 # The size of the blocks the dataset will be split into for testing\n",
        ")\n",
        "\n",
        "# Set up the data collator\n",
        "# Data collator is responsible for batching the data and preparing it for input into the model\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, # The tokenizer used for preparing the data\n",
        "    mlm=False, # Indicates whether masked language modeling is used. False for causal (autoregressive) language modeling with GPT-2\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "# These specify various settings and hyperparameters for the training process\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results', # Directory where the training outputs (like model checkpoints) will be saved\n",
        "    overwrite_output_dir=True, # If True, overwrite the contents of the output directory if it already exists\n",
        "    num_train_epochs=3, # The total number of training epochs (complete passes through the dataset)\n",
        "    per_device_train_batch_size=4, # Batch size per device during training\n",
        "    per_device_eval_batch_size=4, # Batch size for evaluation\n",
        "    eval_steps=400, # Perform an evaluation every `eval_steps` steps\n",
        "    save_steps=800, # Save a model checkpoint every `save_steps` steps\n",
        "    warmup_steps=500, # Number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True, # When True, only return the loss; otherwise, also return logits and more during evaluation\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model, # The model to be trained\n",
        "    args=training_args, # The training arguments\n",
        "    data_collator=data_collator, # The data collator\n",
        "    train_dataset=train_dataset, # The training dataset\n",
        "    eval_dataset=test_dataset, # The evaluation (testing) dataset\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "raer41rXF7tp",
        "outputId": "233bef5e-c2cc-413c-9532-679b65f22936"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=2.9471543629964194, metrics={'train_runtime': 1.197, 'train_samples_per_second': 15.038, 'train_steps_per_second': 5.013, 'total_flos': 1175814144000.0, 'train_loss': 2.9471543629964194, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer for future use\n",
        "model_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_model_gpt2\"\n",
        "tokenizer_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2\"\n",
        "\n",
        "# Saving the model\n",
        "model.save_pretrained(model_save_path)\n",
        "# Saving the tokenizer associated with the model\n",
        "tokenizer.save_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39C0vKaiEW1N",
        "outputId": "563381a7-a647-4925-a671-7998368f8df3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/vocab.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/merges.txt',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#After saving the fine-tuned model and tokenizer, you can load them back using the from_pretrained method. Hereâ€™s how you do it:\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_model_gpt2\"\n",
        "tokenizer_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2\"\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_save_path)\n",
        "\n",
        "# Ensure the tokenizer uses the correct padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "HsuATz5WEbHq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "user_query = \"What's the temperature of the sun?\"\n",
        "\n",
        "input_ids = tokenizer.encode(user_query, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "# Generate a response using the model with adjusted parameters\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=200,  # Adjust as needed for longer responses\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    temperature=0.3,  # Adjust for creativity\n",
        "    top_k=50,\n",
        "    top_p=0.92,\n",
        "    repetition_penalty=1.2,\n",
        "    no_repeat_ngram_size=2,\n",
        "    do_sample=True  # Important for using temperature, top_k, and top_p\n",
        ")\n",
        "\n",
        "# Decode the generated response\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NIS-9dZOp3q",
        "outputId": "2cef490b-35f1-4db9-a5db-c7ac28cd11cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: What's the temperature of the sun?\n",
            "The temperatures are not exactly what you'd expect from a planet with an average annual mean surface area (AAP). The Aap is just one measure that measures how much heat energy gets absorbed by Earth. So, for example if we were to take our current climate and put it into perspective: If there was no solar activity at all in 2012-2013 then this would be about 1/3 as hot per year compared between 2011 & 2013 - which means 2 times more CO2 than today! This isn't even close enough... but let's go back further because now I'm sure some people will say \"well why don' t they have better data?\" Well yes indeed!! We can see here on NASA website http://www1.nasaearthquakes.org/. And look up their page where most scientists agree when looking at global warming rates over time based upon satellite measurements or other sources such like weather stations etc.. It seems pretty clear these\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}