{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Jb15xPmUET",
        "outputId": "0cc16d8b-0d7a-44f8-a9ba-d4433a8b644a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers[torch] accelerate -U\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained model and tokenizer (GPT2 from Hugging_Face)\n",
        "model_name = 'gpt2'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl7uZ3o3E_E1",
        "outputId": "b79bbb4d-eda6-46fc-8d92-56ec9a8abca4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for training and testing\n",
        "train_path = '/content/drive/My Drive/Colab_Notebooks/datasets/train_data.jsonl'\n",
        "test_path = '/content/drive/My Drive/Colab_Notebooks/datasets/test_data.jsonl'"
      ],
      "metadata": {
        "id": "cjhTZ-bfE9l2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the training and test files\n",
        "# TextDataset is a utility from transformers that reads and formats the data file for training\n",
        "train_dataset = TextDataset(\n",
        "  tokenizer=tokenizer, # The tokenizer used to process the text\n",
        "  file_path=train_path, # The path to the training data\n",
        "  block_size=128 # The size of the blocks the dataset will be split into for training\n",
        ")\n",
        "\n",
        "test_dataset = TextDataset(\n",
        "  tokenizer=tokenizer, # The tokenizer used to process the text\n",
        "  file_path=test_path, # The path to the testing data\n",
        "  block_size=128 # The size of the blocks the dataset will be split into for testing\n",
        ")\n",
        "\n",
        "# Set up the data collator\n",
        "# Data collator is responsible for batching the data and preparing it for input into the model\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, # The tokenizer used for preparing the data\n",
        "    mlm=False, # Indicates whether masked language modeling is used. False for causal (autoregressive) language modeling with GPT-2\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "# These specify various settings and hyperparameters for the training process\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results', # Directory where the training outputs (like model checkpoints) will be saved\n",
        "    overwrite_output_dir=True, # If True, overwrite the contents of the output directory if it already exists\n",
        "    num_train_epochs=3, # The total number of training epochs (complete passes through the dataset)\n",
        "    per_device_train_batch_size=4, # Batch size per device during training\n",
        "    per_device_eval_batch_size=4, # Batch size for evaluation\n",
        "    eval_steps=400, # Perform an evaluation every `eval_steps` steps\n",
        "    save_steps=800, # Save a model checkpoint every `save_steps` steps\n",
        "    warmup_steps=500, # Number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True, # When True, only return the loss; otherwise, also return logits and more during evaluation\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model, # The model to be trained\n",
        "    args=training_args, # The training arguments\n",
        "    data_collator=data_collator, # The data collator\n",
        "    train_dataset=train_dataset, # The training dataset\n",
        "    eval_dataset=test_dataset, # The evaluation (testing) dataset\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "raer41rXF7tp",
        "outputId": "a32ecd62-672c-451c-e14f-f897f650c905"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:53, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=2.9371121724446616, metrics={'train_runtime': 68.5845, 'train_samples_per_second': 0.262, 'train_steps_per_second': 0.087, 'total_flos': 1175814144000.0, 'train_loss': 2.9371121724446616, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer for future use\n",
        "model_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_model_gpt2\"\n",
        "tokenizer_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2\"\n",
        "\n",
        "# Saving the model\n",
        "model.save_pretrained(model_save_path)\n",
        "# Saving the tokenizer associated with the model\n",
        "tokenizer.save_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39C0vKaiEW1N",
        "outputId": "95fe3576-548e-4d36-819c-2b3f60de3e1f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/vocab.json',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/merges.txt',\n",
              " '/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#After saving the fine-tuned model and tokenizer, you can load them back using the from_pretrained method. Here’s how you do it:\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_model_gpt2\"\n",
        "tokenizer_save_path = \"/content/drive/My Drive/Colab_Notebooks/fine_tuned_tokenizer_gpt2\"\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_save_path)\n"
      ],
      "metadata": {
        "id": "HsuATz5WEbHq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing Inference\n",
        "\n",
        "# User query\n",
        "user_query = \"What's the temperature of mars?\"\n",
        "\n",
        "# Encode the user query using the tokenizer. Add the EOS token as GPT-2 requires.\n",
        "input_ids = tokenizer.encode(user_query + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "# Generate a response using the model. Adjust `max_length` as needed.\n",
        "output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Decode the generated response\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNdNfpZrFAEf",
        "outputId": "699aebd9-0f89-42d5-df13-78552c075203"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: What's the temperature of mars?The first time I saw the new \"The Walking Dead\" trailer, I was so excited. I was so excited to see the first trailer for the upcoming season of the show. I was so excited to see the first trailer for the upcoming season of the show.\n",
            "\n",
            "I was so excited to see the first trailer for the upcoming season of the show. I was so excited to see the first trailer for the upcoming season of the show.\n",
            "\n",
            "I\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}