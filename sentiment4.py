# -*- coding: utf-8 -*-
"""Sentiment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvHAGhHhoaQCt5LAoioz2OqrSXDtG6m-
"""

#Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
import pickle
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from IPython.display import HTML
import numpy as np
from sklearn import preprocessing
from keras.datasets import imdb
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, GlobalAveragePooling1D, Dense, SpatialDropout1D, LSTM, Dropout
from keras.optimizers import SGD
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical

# Define max dictionary words and max lenght of each review
num_words = 12000
max_phrase_len = 256

# Import the IMDB movie reviews
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)

# Display number of training and testing data
print("Training entries: {}, labels: {}".format(len(X_train), len(y_train)))

# Insert special words (START, PAD, UNKNOWN, UNUSED)
INDEX_FROM=3
word_to_id = imdb.get_word_index()
word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}
word_to_id["<PAD>"] = 0
word_to_id["<START>"] = 1
word_to_id["<UNK>"] = 2
word_to_id["<UNUSED>"] = 3

id_to_word = {value:key for key,value in word_to_id.items()}

# Negative review example
print(X_train[2])
print(' '.join(id_to_word[id] for id in X_train[2] ))

# Positive review example
print(X_train[0])
print(' '.join(id_to_word[id] for id in X_train[0] ))

# Display the target label
# 0 = negative, 1 = positive
print(y_train[0])

# Align the lenght of each training and test review to 256 by adding PAD words

X_train = pad_sequences(
    X_train,
    value=word_to_id["<PAD>"],
    padding='post',
    maxlen=max_phrase_len
)
X_test = pad_sequences(
    X_test,
    value=word_to_id["<PAD>"],
    padding='post',
    maxlen=max_phrase_len
)

len(X_train[0]), len(X_train[1])

# Display the result after the Padding process (words)
print(' '.join(id_to_word[id] for id in X_train[1] ))

# Display the result after the Padding process (tokenized)
print(X_train[1])

# Create the RNN Neural Network

model_lstm = Sequential()
model_lstm.add(Embedding(input_dim = num_words, output_dim = 32, input_length = max_phrase_len))
model_lstm.add(SpatialDropout1D(0.1))
model_lstm.add(LSTM(32, dropout = 0.2, recurrent_dropout = 0.2,return_sequences=True))
model_lstm.add(LSTM(32))
model_lstm.add(Dense(128, activation = 'relu'))
model_lstm.add(Dropout(0.2))
model_lstm.add(Dense(2, activation = 'softmax'))

model_lstm.summary()

# Compile the RNN model

model_lstm.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='Adam',
    metrics=['accuracy']
)

# Fit our model

history = model_lstm.fit(
    X_train,
    y_train,
    validation_split = 0.2,
    epochs = 20,
    batch_size = 64
)

#Plot the Loss
plt.clf()
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'y', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Plot the accuracy
plt.clf()
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'g', label='Training acc')
plt.plot(epochs, val_acc, 'y', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Save the model to disk
model_lstm.save('finalized_model_sentiment.h5')

# Display the review to be predicted
# 0=Negative
# 1=Positive
review_index = 2
y_test[review_index]

#Sentiment Prediction
#Make the inference by using the fitted RNN model
pred = model_lstm.predict(X_test[review_index].reshape(-1,256))
print("\n\033[1mPredicted sentiment [0=Negative - 1=Positive]: %.0f"%pred.argmax(),"\033[0m \n ")
print("Predicted probability array:")
print(pred)

# Display the review to be predicted
# 0=Negative
# 1=Positive

review_index = 3
y_test[review_index]

#Sentiment Prediction
#Make the inference by using the fitted RNN model
pred = model_lstm.predict(X_test[review_index].reshape(-1,256))

print("\n\033[1mPredicted sentiment [0=Negative - 1=Positive]: %.0f"%pred.argmax(),"\033[0m \n ")
print("Predicted probability array:")
print(pred)

